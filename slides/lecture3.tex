\documentclass[handout,aspectratio=169]{beamer}
\usetheme{default}

\usepackage{amsmath,amssymb}

\newcommand{\pitem}{\pause\item}

\newcommand{\bits}{\{0,1\}}
\newcommand{\bitstr}{\bits^*}
\newcommand{\sshalf}{{\textstyle\frac12}}
\newcommand{\seqn}[2]{{#1}_1,{#1}_2,\dotsc,{#1}_{#2}}
\newcommand{\seqin}[3]{{#1}_{{#2}_1},{#1}_{{#2}_2},\dotsc,{#1}_{{#2}_{#3}}}
\newcommand{\IC}{\mathrm{IC}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\Nat}{\mathbb{N}}

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\rng}{rng}
\DeclareMathOperator*{\E}{\mathbb{E}}

\newenvironment{sol}{%
    \begin{block}{}%
        \setbeamercolor{block body}{bg=lightgray}%
    }{
\end{block}}

\AtBeginSection[]{
    \begin{frame}
        \vfill
        \centering
        \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
            \usebeamerfont{title}\insertsectionhead\par%
        \end{beamercolorbox}
        \vfill
    \end{frame}
}


\title{Coding}
\author{Alexander Smal at NUP}
\date{23.10.2024.\\ Lecture 3}

\begin{document}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}[plain]
        \maketitle
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \section{Coding}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Uniquely Decodable Codes}
        Let $\mathcal{A} = \{\seqn{a}{n}\}$ be an \emph{alphabet}.
        \begin{definition}
            A \emph{code} is a function \(C:\mathcal{A}\to\bitstr\),
            which assigns \emph{codewords} to the \emph{letters} of the alphabet.

            If every message obtained by applying the code \(C\) can be decoded
            uniquely, then the code is called \emph{uniquely decodable}.
        \end{definition}

        \begin{theorem}[Kraft-McMillan Inequality]\label{thm:mcmill}
            For every uniquely decodable code with codewords
            \(\{\seqn{c}{n}\}\),
            \[
            \sum_{i=1}^{n} 2^{-|c_i|} \le 1.
            \]
        \end{theorem}
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Prefix-free codes}
        \begin{definition}
            A code is called \emph{prefix (prefix-free)}, if no
            codeword is a prefix of another codeword.
        \end{definition}
        \pause
        \begin{theorem}[Kraft-McMillan Inequality for prefix codes]\label{thm:mcmill}
            For prefix code with codewords
            \(\{\seqn{c}{n}\}\), $\sum_{i=1}^{n} 2^{-|c_i|} \le 1.$
        \end{theorem}
        \pause
        \begin{theorem}
            For a set of integers
            \(\{\seqn{\ell}{n}\}\)
            that satisfies
            $
            \sum_{i=1}^{n} 2^{-\ell_i} \le 1,
            $
            there exists a prefix code with codewords \(\{\seqn{c}{n}\}\), where \(|c_i| = \ell_i\).
        \end{theorem}
        \pause
        \begin{corollary}
            For any uniquely decodable code, there exists a prefix code with the same
            lengths of codewords.
        \end{corollary}
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Codes and Shannon Entropy: lower bound}
        \begin{theorem}[Shannon]
            For any uniquely decodable code, the following holds:
            \[
            \sum_{i=1}^n p_i \cdot |c_i| \ge \sum_{i=1}^n p_i \cdot \log \frac{1}{p_i}.
            \]
        \end{theorem}
        \begin{proof}
            Move everything to the right side and apply Jensen's inequality:
            \[
            \sum_{i=1}^n p_i \cdot \log \frac{2^{-|c_i|}}{p_i} \le
            \log \sum_{i=1}^n \left(p_i \frac{2^{-|c_i|}}{p_i}\right) =
            \log \sum_{i=1}^n 2^{-|c_i|} \le \log 1 = 0.
            \]
        \end{proof}
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Codes and Shannon Entropy: upper bound}
        \begin{theorem}[Shannon]\label{thm:shannon:optcode}
            For every distribution \(\{\seqn{p}{n}\}\), there exists
            a prefix code \(\{\seqn{c}{n}\}\):
            \[
            \sum_{i=1}^n p_i \cdot |c_i| \le \sum_{i=1}^n p_i \cdot \log \frac{1}{p_i} + 1.
            \]
        \end{theorem}\vspace{-5mm}
        \begin{proof}
            Let \(\ell_i = \bigl\lceil \log \frac{1}{p_i} \bigr\rceil\). Note that \(\{
            \ell_i\}\) satisfy
            the Kraft-McMillan inequality:
            \[
            \sum_{i=1}^n 2^{-|c_i|} =
            \sum_{i=1}^n 2^{-\lceil \log \frac{1}{p_i} \rceil} \le
            \sum_{i=1}^n 2^{-\log \frac{1}{p_i}} =
            \sum_{i=1}^n p_i = 1.
            \]
            Lets estimate the average code length:
            \[
            \sum_{i=1}^n p_i \cdot |c_i| =
            \sum_{i=1}^n p_i \cdot \bigl\lceil \log{\textstyle\frac{1}{p_i}} \bigr\rceil <
            \sum_{i=1}^n p_i \cdot \bigl(\log{\textstyle\frac{1}{p_i}} + 1\bigr) =
            \left(\sum_{i=1}^n p_i \cdot \log{\textstyle\frac{1}{p_i}}\right) + 1.
            \]
        \end{proof}
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Summary}
        \begin{enumerate}
            \item For every uniquely decodable code there is a prefix code with
            the same code lengh.
            \item The average code length of the optimal code is related to Shannon Entropy:
            \[
            H(\seqn{p}{n}) \le \sum_{i=1}^{n} p_i\cdot |c_i| \le H(\seqn{p}{n}) +1.
            \]
            \item
            The '+1' on the right side cannot be eliminated: for example, if we have only two symbols in
            the alphabet, then \(\sum p_i \cdot |c_i| = 1\), while \(\sum
            p_i \log \frac{1}{p_i}\) can be arbitrarily close to zero.

        \end{enumerate}
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Examples}
        \begin{block}{Shannon-Fano Code}
            Assume that \(p_1 \ge p_2 \ge \dotsb \ge p_n\) and assign it to the subsegments of $[0,1]$. \\ All codes in $[0,1/2]$ start with $0$, all other start with $1$. Continue recursively. \vspace{2cm}

        \end{block}
        \begin{block}{Huffman Code}
            Assume that \(p_1 \ge p_2 \ge \dotsb \ge p_n\). Use gready algorithm to build Huffman Tree.
            \vspace{3cm}

        \end{block}
    \end{frame}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Block Coding}
        Now we encode not individual symbols, but blocks of symbols.
        Let each block consist of \(k\) symbols. Let the random variables \(\seqn{\alpha}{k}\) be distributed as \(\alpha\) and correspond to the letters in the block.
        \[
        H(\seqn{\alpha}{k}) = \sum_{i=1}^k H(\alpha_i) = k \cdot H(\alpha).
        \]
        Due to Shannon's theorems, we have the following:
        \[
        H(\alpha) \le (\text{average length of the letter code in the block}) \le H(\alpha) +
        \frac{1}{k}.
        \]
        When coding blocks of length 100, we achieve a deviation from entropy of no more than 0.01. However, we cannot apply the Huffman code since the algorithm for its construction would require passing \(n^{100}\) frequencies of the symbols.
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Arithmetic Coding}



        \begin{definition}
            Assume that \(p_1 \ge p_2 \ge \dotsb \ge p_n\) and assign it to the subsegments of $[0,1]$.

            Repeat it recursively for every subsegment for depth $k$.

            Half-open interval is called \emph{standard} if it has the form
            \([0.v0_2, 0.v1_2)\), where \(v\in\bits^*\).

            We will assign to each standard interval \([0.v0_2, 0.v1_2)\) the code \(v0\).
            \vspace{5cm}


        \end{definition}
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Arithmetic Coding: properties}
        Arithmetic coding is not as optimal:
        $
        \sum_{i=1}^n p_i \cdot |c_i| \le \sum_{i=1}^n p_i \cdot \log \frac{1}{p_i} + 2,
        $
        \pause
        \begin{theorem}
            For a half-open interval \([a,b)\), there is a standard interval of length \(2^{-k}\), such that
            \(
            \frac{b-a}{4} < 2^{-k} \le \frac{b-a}{2},
            \)
            Thus, the length of the code for any interval does not exceed \(\log \frac{4}{b-a} = \log \frac{1}{p} + 2\), where \(p\) is the probability of the corresponding block.
        \end{theorem}
        \vspace{3cm}

    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Block Coding with Errors}
        Let \(\seqn{\alpha}{n}\) be i.i.d. random variables on \(\{\seqn{a}{k}\}\) with probabilities \(\seqn{p}{k}\). Consider block coding defined by functions \(E_n\) and \(D_n\):
        \[
        E_n:\{\seqn{a}{k}\}^n \to \{0,1\}^{L_n},\qquad
        D_n:\{0,1\}^{L_n} \to \{\seqn{a}{k}\}^n.
        \]

            The \emph{error probability \(\varepsilon_n\)} is the probability of the following event:
            \[
            [(\seqn{\alpha}{n}) = (\seqin{a}{i}{n}) \mid D_n(E_n(\seqin{a}{i}{n}))\neq (\seqin{a}{i}{n})].
            \]
        \begin{theorem}[Shannon]\label{thm:blockcoding}
            \begin{enumerate}
                \item If \(h > H(\alpha) = \sum_{i=1}^{k} p_i \log \frac{1}{p_i}\), then there exist functions \((E_n, D_n)\) for \(L_n = \lceil h \cdot n \rceil\), such that \(\varepsilon_n \to 0\) as \(n \to \infty\).

                \item If \(h < H(\alpha) = \sum_{i=1}^{k} p_i \log \frac{1}{p_i}\), then for any functions \((E_n, D_n)\) for \(L_n = \lceil h \cdot n \rceil\),\\ the error probability \(\varepsilon_n \to 1\) as \(n \to \infty\).
            \end{enumerate}
        \end{theorem}
    \end{frame}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
